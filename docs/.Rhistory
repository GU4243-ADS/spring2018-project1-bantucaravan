theme(legend.position = "none")
ggplot(most_freq) +
geom_col(aes(reorder(word, all), n, fill = author)) +
xlab("awesome") +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
ggplot(most_freq) +
geom_col(aes(reorder(word, all), n, fill = author), labs) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
ggplot(spooky) +
geom_density(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
?geom_density_ridges
ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
spooky_wrd$word_length <- str_length(spooky_wrd$word)
head(spooky_wrd$word_length)
ggplot(spooky_wrd) +
geom_density(aes(word_length, fill = author), bw = 0.05, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Word length [# characters]")
count(spooky_wrd, author, word)
count(group_by(spooky_wrd, word, author))
count(spooky_wrd, word, author)
?ungroup
spooky$author <- as.factor(spooky$author)
# tokenize by word. I didn't include the elimination of the stop words because for tf_idf they will be irrelevant.
spooky_wrd <- unnest_tokens(spooky, word, text)
# This eliminates the most frequent by contexually insignificant words --> removes rows of spooky_wrd that match in column word with the stopwords list.
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
author_words <- count(spooky_wrd, word, author)
all_words    <- rename(count(spooky_wrd, word), all = n)
head(all_words)
author_words <- left_join(author_words, all_words, by = "word")
#organizes data from highest frequency to lowest --> orders the tibble by the "all" column from largest to smallest
author_words <- arrange(author_words, desc(all))
head(author_words)
most_freq <- head(author_words, 81)
ggplot(most_freq) +
geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
?count
count(spooky_wrd, author, word)
?head
?top_n
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# set author column to "factor"
## Why did we use as.factor() not factor()? would there have been a difference? A: factor() would have required creating a new object, this just converted part of our already existing object.
spooky$author <- as.factor(spooky$author)
# tokenize by word. I didn't include the elimination of the stop words because for tf_idf they will be irrelevant.
spooky_wrd <- unnest_tokens(spooky, word, text)
# This eliminates the most frequent by contexually insignificant words --> removes rows of spooky_wrd that match in column word with the stopwords list.
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used. *group_by is unnecessary in following lines*
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
# adds column of the number of times the word appears in the corpus overall corresponding to each occurence of a word in a row
author_words <- left_join(author_words, all_words, by = "word")
#organizes data from highest frequency to lowest --> orders the tibble by the "all" column from largest to smallest
author_words <- arrange(author_words, desc(all))
head(author_words)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# set author column to "factor"
## Why did we use as.factor() not factor()? would there have been a difference? A: factor() would have required creating a new object, this just converted part of our already existing object.
spooky$author <- as.factor(spooky$author)
# tokenize by word. I didn't include the elimination of the stop words because for tf_idf they will be irrelevant.
spooky_wrd <- unnest_tokens(spooky, word, text)
# This eliminates the most frequent by contexually insignificant words --> removes rows of spooky_wrd that match in column word with the stopwords list.
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used. *group_by is unnecessary in following lines*
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
# adds column of the number of times the word appears in the corpus overall corresponding to each occurence of a word in a row
author_words <- left_join(author_words, all_words, by = "word")
#organizes data from highest frequency to lowest --> orders the tibble by the "all" column from largest to smallest
author_words <- ungroup(arrange(author_words, desc(all)))
head(author_words)
?top_n
most_freq <- top_n(group_by(author_words, author),30,n)
head(most_freq)
most_freq
tail(most_freq)
most_freq <- ungroup(top_n(group_by(author_words, author),30,n))
tail(most_freq)
count(most_freq, author)
count(author_words, author)
count <- count(author_words, author)
left_join(author_words, count, by = author)
head(author_words)
count
?left_join
left_join(author_words, count, by = author)
left_join(author_words, count, by = "author")
author_words <- left_join(author_words, count, by = "author")
rename(author_words, nn = base)
?rename
rename(author_words, base <- nn)
rename(author_words, base = nn)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# set author column to "factor"
## Why did we use as.factor() not factor()? would there have been a difference? A: factor() would have required creating a new object, this just converted part of our already existing object.
spooky$author <- as.factor(spooky$author)
# tokenize by word. I didn't include the elimination of the stop words because for tf_idf they will be irrelevant.
spooky_wrd <- unnest_tokens(spooky, word, text)
# This eliminates the most frequent by contexually insignificant words --> removes rows of spooky_wrd that match in column word with the stopwords list.
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used. *group_by is unnecessary in following lines*
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
# adds column of the number of times the word appears in the corpus overall corresponding to each occurence of a word in a row
author_words <- left_join(author_words, all_words, by = "word")
#organizes data from highest frequency to lowest --> orders the tibble by the "all" column from largest to smallest
author_words <- ungroup(arrange(author_words, desc(all)))
mutate(group_by(author_words, author), freqn = n/count(author_words, author))
mutate(group_by(author_words, author), freqn = n/length(n))
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# set author column to "factor"
## Why did we use as.factor() not factor()? would there have been a difference? A: factor() would have required creating a new object, this just converted part of our already existing object.
spooky$author <- as.factor(spooky$author)
# tokenize by word. I didn't include the elimination of the stop words because for tf_idf they will be irrelevant.
spooky_wrd <- unnest_tokens(spooky, word, text)
# This eliminates the most frequent by contexually insignificant words --> removes rows of spooky_wrd that match in column word with the stopwords list.
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used. *group_by is unnecessary in following lines*
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
# adds column of the number of times the word appears in the corpus overall corresponding to each occurence of a word in a row
author_words <- left_join(author_words, all_words, by = "word")
#organizes data from highest frequency to lowest --> orders the tibble by the "all" column from largest to smallest
author_words <- ungroup(arrange(author_words, desc(all)))
count <- count(author_words, author)
author_words <- left_join(author_words, count, by = "author")
mutate(author_words, freqn = n/nn)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# set author column to "factor"
## Why did we use as.factor() not factor()? would there have been a difference? A: factor() would have required creating a new object, this just converted part of our already existing object.
spooky$author <- as.factor(spooky$author)
# tokenize by word. I didn't include the elimination of the stop words because for tf_idf they will be irrelevant.
spooky_wrd <- unnest_tokens(spooky, word, text)
# This eliminates the most frequent by contexually insignificant words --> removes rows of spooky_wrd that match in column word with the stopwords list.
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used. *group_by is unnecessary in following lines*
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
# adds column of the number of times the word appears in the corpus overall corresponding to each occurence of a word in a row
author_words <- left_join(author_words, all_words, by = "word")
#organizes data from highest frequency to lowest --> orders the tibble by the "all" column from largest to smallest
author_words <- ungroup(arrange(author_words, desc(all)))
#adds a colum which is the frequency of a words occurence within given author's sentences.
author_words <- mutate(group_by(author_words, author), freqn = n/length(n))
# delete, same as above
#count <- count(author_words, author)
#author_words <- left_join(author_words, count, by = "author")
#mutate(author_words, freqn = n/nn)
# Top 30 within-author most frequent words for each author
most_freq <- ungroup(top_n(group_by(author_words, author),30,freqn))
#
ggplot(most_freq) +
geom_col(aes(word, freqn, fill = author)) +
labs(x = NULL, y = "frequency") +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values")
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values")
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3) +
coord_flip() +
labs(y = "TF-IDF values")
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values", x = NULL)
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
coord_flip() +
labs(y = "TF-IDF values", x = NULL)
most_freq
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values", x = NULL)
author_words
ungroup(author_words)
author_words <- ungroup(author_words)
most_freq <- ungroup(top_n(group_by(author_words, author),30,freqn))
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
labs(y = "Frequency", x = NULL)
reorder(word, freqn)
?reorder
arrange(most_freq)
arrange(most_freq, desc(freqn))
ggplot(arrange(most_freq, desc(freqn))) +
geom_col(aes(word, freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
arrange(most_freq, desc(freqn))
ggplot(arrange(most_freq, desc(freqn))) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
?reorder
type.of(most_freq$word)
typeof(author_words$word)
?arrange()
arrange(most_freq, desc(freqn))
arrange(most_freq, desc(freqn))[,1]
ggplot(most_freq) +
geom_col(aes(arrange(most_freq, desc(freqn))[,1], freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
arrange(most_freq, desc(freqn))$word
ggplot(most_freq) +
geom_col(aes(arrange(most_freq, desc(freqn))$word, freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
?sort
order(most_freq)
sort(most_freq)
?as.factor
arrange(most_freq, desc(freqn))
x <- as.factor(arrange(most_freq, desc(freqn))$author)
x
x <- arrange(most_freq, desc(freqn))
x
x$word <- as.factor(arrange(most_freq, desc(freqn))$word)
x$word <- as.factor(arrange(most_freq, desc(freqn))$word, ordered = TRUE)
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author, ncol = 3, scales = "free") +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author, ncol = 3, scales = "free") +
theme(legend.position = "none")
x$word <- as.factor(group_by(arrange(most_freq, desc(freqn)), author)$word)
x
ggplot(x) +
geom_col(aes(reorder(word, freqn), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author, ncol = 3, scales = "free") +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none")+
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none")+
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free")
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author, ncol = 3, scales = "free") +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author, ncol = 3, scales = "free") +
theme(legend.position = "none")
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), frequency(), fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author, ncol = 3, scales = "free") +
theme(legend.position = "none")
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = sum), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
?scale_x_discrete()
?reorder
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, function(x)-length(x)), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, function(x)-length(x)), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
factor?
?factor
?factor
?str
?order
x
x$word <- factor(x$word, levels = x$word[order(x$freqn)])
ggplot(group_by(most_freq, author) +
geom_col(aes(reorder(words, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(group_by(most_freq, author)) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(group_by(most_freq, author)) +
geom_col(aes(reorder(word, freqn, FUN = sum), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
x <- group_by(most_freq, author)
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = sum), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(x) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
ggplot(most_freq) +
geom_col(aes(reorder(word, freqn, FUN = min), freqn, fill = author)) +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency", x = NULL)
#What is knitr:: command? Note this is a "setup" chunk.
knitr::opts_knit$set(root.dir = normalizePath('../'))
# load the libraries
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
source("/libs/multiplot.R")
source("../libs/multiplot.R")
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("/data/spooky.csv", as.is = TRUE)
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
#What is knitr:: command? Note this is a "setup" chunk.
knitr::opts_knit$set(root.dir = normalizePath('../'))
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("../data/spooky.csv", as.is = TRUE)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("/data/spooky.csv", as.is = TRUE)
# read in our data --> ".." references the parent directory to the current wd
spooky <- read.csv("data/spooky.csv", as.is = TRUE)
# set author column to "factor"
## Why did we use as.factor() not factor()? would there have been a difference? A: factor() would have required creating a new object, this just converted part of our already existing object.
spooky$author <- as.factor(spooky$author)
# tokenize by word. I didn't include the elimination of the stop words because for tf_idf they will be irrelevant.
spooky_wrd <- unnest_tokens(spooky, word, text)
# This eliminates the most frequent by contexually insignificant words --> removes rows of spooky_wrd that match in column word with the stopwords list.
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Counts number of times each author used each word.
author_words <- count(spooky_wrd, word, author)
# Counts number of times each word was used. *group_by is unnecessary in following lines*
all_words    <- rename(count(spooky_wrd, word), all = n)
# adds column of the number of times the word appears in the corpus overall corresponding to each occurence of a word in a row
author_words <- left_join(author_words, all_words, by = "word")
#organizes data from highest frequency to lowest --> orders the tibble by the "all" column from largest to smallest
author_words <- arrange(author_words, desc(all))
